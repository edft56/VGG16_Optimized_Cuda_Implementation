{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.utils import data_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import randaugment\n",
    "\n",
    "# need to have imagenet training and validation files in TFRecord format. \n",
    "\n",
    "SO_ABS_FILEPATH = \"\"\n",
    "IMAGENET_TRAIN_FILEPATH = \"\"\n",
    "IMAGENET_VAL_FILEPATH = \"\"\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      \n",
    "with tf.device('/cpu:0'):\n",
    "    vgg16_module = tf.load_op_library(SO_ABS_FILEPATH)\n",
    "\n",
    "print(tf.version.VERSION)\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))\n",
    "print(tf.test.is_built_with_gpu_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_filenames(base_directory = IMAGENET_TRAIN_FILEPATH):\n",
    "\n",
    "    train_filenames = []\n",
    "\n",
    "    for i in range(1024):\n",
    "        if i<10:\n",
    "            train_filenames.append(base_directory + \"/train-0000\"+str(i)+\"-of-01024\")\n",
    "        elif i<100:\n",
    "            train_filenames.append(base_directory + \"/train-000\"+str(i)+\"-of-01024\")\n",
    "        elif i<1000:\n",
    "            train_filenames.append(base_directory + \"/train-00\"+str(i)+\"-of-01024\")\n",
    "        else:\n",
    "            train_filenames.append(base_directory + \"/train-0\"+str(i)+\"-of-01024\")\n",
    "\n",
    "    return train_filenames\n",
    "\n",
    "def get_val_filenames(base_directory = IMAGENET_VAL_FILEPATH):\n",
    "\n",
    "    val_filenames = []\n",
    "\n",
    "    for i in range(128):\n",
    "        if i<10:\n",
    "            val_filenames.append(base_directory + \"/validation-0000\"+str(i)+\"-of-00128\")\n",
    "        elif i<100:\n",
    "            val_filenames.append(base_directory + \"/validation-000\"+str(i)+\"-of-00128\")\n",
    "        elif i<1000:\n",
    "            val_filenames.append(base_directory + \"/validation-00\"+str(i)+\"-of-00128\")\n",
    "\n",
    "    return val_filenames\n",
    "\n",
    "\n",
    "def save_weights_to_file (filename, weights, weights_vel, bias, bias_vel):\n",
    "        \n",
    "    weights_hdf5 = h5py.File(filename, 'w')\n",
    "    weights_hdf5.create_dataset('weights', data = weights.numpy())\n",
    "    weights_hdf5.create_dataset('weights_vel', data = weights_vel.numpy())\n",
    "    weights_hdf5.create_dataset('bias', data = bias.numpy())\n",
    "    weights_hdf5.create_dataset('bias_vel', data = bias_vel.numpy())\n",
    "    weights_hdf5.close()\n",
    "\n",
    "def read_weights_from_file(filename):\n",
    "        \n",
    "    file = h5py.File(filename, 'r')\n",
    "    weights = file['weights']\n",
    "    weights_vel = file['weights_vel']\n",
    "    bias = file['bias']\n",
    "    bias_vel = file['bias_vel']\n",
    "    \n",
    "    weights = tf.convert_to_tensor(weights[...],tf.float32)\n",
    "    weights_vel = tf.convert_to_tensor(weights_vel[...],tf.float32)\n",
    "    bias = tf.convert_to_tensor(bias[...],tf.float32)\n",
    "    bias_vel = tf.convert_to_tensor(bias_vel[...],tf.float32)\n",
    "    \n",
    "    file.close()\n",
    "\n",
    "    return weights, weights_vel, bias, bias_vel\n",
    "\n",
    "\n",
    "\n",
    "image_feature_description = {\n",
    "                                'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "                                'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "                                'image/colorspace': tf.io.FixedLenFeature([], tf.string),\n",
    "                                'image/channels': tf.io.FixedLenFeature([], tf.int64),\n",
    "                                'image/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "                                'image/class/synset': tf.io.FixedLenFeature([], tf.string),\n",
    "                                'image/class/text': tf.io.FixedLenFeature([], tf.string),\n",
    "                                'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "                                'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "                                'image/encoded': tf.io.FixedLenFeature([], tf.string)\n",
    "                                }\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def _parse_single_image_function(example_proto):\n",
    "    \n",
    "    # Parse the input tf.Example proto using the given dictionary.\n",
    "    return tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "\n",
    "#function found in TF source, possibly modified a bit\n",
    "def distorted_bounding_box_crop(image,\n",
    "                                bbox = [[[0,0,1.0,1.0]]],\n",
    "                                min_object_covered=0.65,\n",
    "                                aspect_ratio_range=(0.75, 1.33),\n",
    "                                area_range=(0.65, 0.85),\n",
    "                                max_attempts=10,\n",
    "                                ):\n",
    "    # Generates cropped_image using one of the bboxes randomly distorted.\n",
    "    # See `tf.image.sample_distorted_bounding_box` for more documentation.\n",
    "    # Args:\n",
    "    # image_bytes: `Tensor` of binary image data.\n",
    "    # bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n",
    "    #     where each coordinate is [0, 1) and the coordinates are arranged\n",
    "    #     as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n",
    "    #     image.\n",
    "    # min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n",
    "    #     area of the image must contain at least this fraction of any bounding\n",
    "    #     box supplied.\n",
    "    # aspect_ratio_range: An optional list of `float`s. The cropped area of the\n",
    "    #     image must have an aspect ratio = width / height within this range.\n",
    "    # area_range: An optional list of `float`s. The cropped area of the image\n",
    "    #     must contain a fraction of the supplied image within in this range.\n",
    "    # max_attempts: An optional `int`. Number of attempts at generating a cropped\n",
    "    #     region of the image of the specified constraints. After `max_attempts`\n",
    "    #     failures, return the entire image.\n",
    "    # scope: Optional `str` for name scope.\n",
    "    # Returns:\n",
    "    # cropped image `Tensor`\n",
    "    \n",
    "\n",
    "    shape = tf.shape(image)\n",
    "    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n",
    "                                                shape,\n",
    "                                                bounding_boxes=bbox,\n",
    "                                                min_object_covered=min_object_covered,\n",
    "                                                aspect_ratio_range=aspect_ratio_range,\n",
    "                                                area_range=area_range,\n",
    "                                                max_attempts=max_attempts,\n",
    "                                                use_image_if_no_bounding_boxes=True\n",
    "                                                                            )\n",
    "    bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n",
    "\n",
    "    # Crop the image to the specified bounding box.\n",
    "    offset_y, offset_x, _ = tf.unstack(bbox_begin)\n",
    "    target_height, target_width, _ = tf.unstack(bbox_size)\n",
    "    \n",
    "    image = tf.image.crop_to_bounding_box(image, offset_y, offset_x, target_height, target_width)\n",
    "\n",
    "    return image\n",
    "\n",
    "#function found in TF source, possibly modified a bit\n",
    "def _crop(image, offset_height, offset_width, crop_height, crop_width):\n",
    "    # Crops the given image using the provided offsets and sizes.\n",
    "    # Note that the method doesn't assume we know the input image size but it does\n",
    "    # assume we know the input image rank.\n",
    "    # Args:\n",
    "    #   image: an image of shape [height, width, channels].\n",
    "    #   offset_height: a scalar tensor indicating the height offset.\n",
    "    #   offset_width: a scalar tensor indicating the width offset.\n",
    "    #   crop_height: the height of the cropped image.\n",
    "    #   crop_width: the width of the cropped image.\n",
    "    # Returns:\n",
    "    #   the cropped (and resized) image.\n",
    "    # Raises:\n",
    "    #   InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n",
    "    #     less than the crop size.\n",
    "    \n",
    "    original_shape = tf.shape(image)\n",
    "    cropped_shape = tf.stack([crop_height, crop_width, original_shape[2]])\n",
    "    offsets = tf.cast(tf.stack([offset_height, offset_width, 0]),tf.int32)\n",
    "\n",
    "    # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n",
    "    # define the crop size.\n",
    "    image = tf.slice(image, offsets, cropped_shape)\n",
    "    return tf.reshape(image, cropped_shape)\n",
    "\n",
    "#function found in TF source, possibly modified a bit\n",
    "def _central_crop(image_list, crop_height, crop_width):\n",
    "    # Performs central crops of the given image list.\n",
    "    # Args:\n",
    "    #   image_list: a list of image tensors of the same dimension but possibly\n",
    "    #     varying channel.\n",
    "    #   crop_height: the height of the image following the crop.\n",
    "    #   crop_width: the width of the image following the crop.\n",
    "    # Returns:\n",
    "    #   the list of cropped images.\n",
    "    \n",
    "    outputs = []\n",
    "    for image in image_list:\n",
    "      image_height = tf.shape(image)[0]\n",
    "      image_width = tf.shape(image)[1]\n",
    "\n",
    "      offset_height = (image_height - crop_height) / 2\n",
    "      offset_width = (image_width - crop_width) / 2\n",
    "\n",
    "      outputs.append(_crop(image, offset_height, offset_width,\n",
    "                          crop_height, crop_width))\n",
    "    return outputs\n",
    "\n",
    "def _mean_image_subtraction(image, means):\n",
    "    means = tf.reshape(means,[1,1,3])\n",
    "    return tf.math.subtract(image,means)\n",
    "\n",
    "#function found in TF source, possibly modified a bit\n",
    "def _smallest_size_at_least(height, width, smallest_side=256):\n",
    "    # Computes new shape with the smallest side equal to `smallest_side`.\n",
    "    # Computes new shape with the smallest side equal to `smallest_side` while\n",
    "    # preserving the original aspect ratio.\n",
    "    # Args:\n",
    "    #   height: an int32 scalar tensor indicating the current height.\n",
    "    #   width: an int32 scalar tensor indicating the current width.\n",
    "    #   smallest_side: A python integer or scalar `Tensor` indicating the size of\n",
    "    #     the smallest side after resize.\n",
    "    # Returns:\n",
    "    #   new_height: an int32 scalar tensor indicating the new height.\n",
    "    #   new_width: and int32 scalar tensor indicating the new width.\n",
    "    \n",
    "    height = tf.cast(height,tf.float32)\n",
    "    width = tf.cast(width,tf.float32)\n",
    "    smallest_side = tf.cast(smallest_side,tf.float32)\n",
    "\n",
    "    scale = tf.cond(tf.greater(height, width),\n",
    "                    lambda: smallest_side / width,\n",
    "                    lambda: smallest_side / height)\n",
    "    new_height = tf.cast(height * scale,tf.int32)\n",
    "    new_width = tf.cast(width * scale,tf.int32)\n",
    "    return new_height, new_width\n",
    "\n",
    "#function found in TF source, possibly modified a bit\n",
    "def _aspect_preserving_resize(image, smallest_side=256):\n",
    "    # Resize images preserving the original aspect ratio.\n",
    "    # Args:\n",
    "    #   image: A 3-D image `Tensor`.\n",
    "    #   smallest_side: A python integer or scalar `Tensor` indicating the size of\n",
    "    #     the smallest side after resize.\n",
    "    # Returns:\n",
    "    #   resized_image: A 3-D tensor containing the resized image.\n",
    "    \n",
    "\n",
    "    shape = tf.shape(image)\n",
    "    height = shape[0]\n",
    "    width = shape[1]\n",
    "    new_height, new_width = _smallest_size_at_least(height, width, smallest_side)\n",
    "    resized_image = tf.image.resize(image, [new_height, new_width], 'bicubic')\n",
    "    return tf.reshape(resized_image, [new_height, new_width, 3])\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def preprocess_for_eval(image, pretrained_tf_model = False, output_height = 224, output_width =224, resize_side = 256):\n",
    "    # Preprocesses the given image for evaluation.\n",
    "    # Args:\n",
    "    #   image: A `Tensor` representing an image of arbitrary size.\n",
    "    #   output_height: The height of the image after preprocessing.\n",
    "    #   output_width: The width of the image after preprocessing.\n",
    "    #   resize_side: The smallest side of the image for aspect-preserving resizing.\n",
    "    # Returns:\n",
    "    #   A preprocessed image.\n",
    "    \n",
    "\n",
    "    image = tf.io.decode_jpeg(image)\n",
    "    if tf.shape(image)[2] == 1: #assert image is rgb\n",
    "        image = tf.image.grayscale_to_rgb(image)\n",
    "    image = _aspect_preserving_resize(image, resize_side)\n",
    "    image = _central_crop([image], output_height, output_width)[0]\n",
    "    image.set_shape([output_height, output_width, 3])\n",
    "\n",
    "    image = tf.clip_by_value(image, 0.0, 255.0)\n",
    "    image = tf.cast(image,tf.uint8)\n",
    "    image = tf.cast(image,tf.float32)\n",
    "\n",
    "    if ( pretrained_tf_model ): _mean_image_subtraction(image, [123.68, 116.779, 103.939])\n",
    "    else: image = _mean_image_subtraction(image, [124.0, 117.0, 104.0])\n",
    "\n",
    "    if ( pretrained_tf_model ): image = image[..., ::-1]\n",
    "    else: image = image/tf.reshape([58.393, 57.12, 57.375],[1,1,3])\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def training_preprocessing(image):\n",
    "    output_height = 224\n",
    "    output_width = 224\n",
    "    resize_side = 256\n",
    "    \n",
    "    image = tf.io.decode_jpeg(image)\n",
    "    if tf.shape(image)[2] == 1: #make sure image is rgb\n",
    "        image = tf.image.grayscale_to_rgb(image)\n",
    "    \n",
    "    image = distorted_bounding_box_crop(image)\n",
    "    image = _aspect_preserving_resize(image, resize_side)\n",
    "    image = _central_crop([image], output_height, output_width)[0]\n",
    "    image.set_shape([output_height, output_width, 3])\n",
    "    image = tf.clip_by_value(image, 0.0, 255.0)\n",
    "    image = tf.cast(image,tf.uint8)\n",
    "\n",
    "    image = randaugment.distort_image_with_randaugment(image, magnitude=20)\n",
    "    image = tf.cast(image,tf.float32)\n",
    "\n",
    "    #image = _mean_image_subtraction(image, [123.68, 116.779, 103.939]) #RGB CHANNELS\n",
    "    image = _mean_image_subtraction(image, [124.0, 117.0, 104.0])\n",
    "    image = image/tf.reshape([58.393, 57.12, 57.375],[1,1,3]) #divide by std\n",
    "    return image\n",
    "    \n",
    "\n",
    "def parse_and_preprocess_training_image(example_proto, tf_vgg16 = False): # @tf.function here seems slower\n",
    "    parsed_example = _parse_single_image_function(example_proto)\n",
    "    image = parsed_example['image/encoded']\n",
    "    image = training_preprocessing(parsed_example['image/encoded'])\n",
    "\n",
    "    if(tf_vgg16): label = tf.one_hot(parsed_example['image/class/label'] -1, 1000, dtype=tf.float32)\n",
    "    else: label = tf.one_hot(parsed_example['image/class/label'] -1, 1000, dtype=tf.int8)\n",
    "    return (image,label)\n",
    "\n",
    "def parse_and_preprocess_eval_image(example_proto, tf_vgg16 = False, pretrained_tf_model = False):\n",
    "    parsed_example = _parse_single_image_function(example_proto)\n",
    "\n",
    "    image = preprocess_for_eval(parsed_example['image/encoded'], pretrained_tf_model)\n",
    "\n",
    "    if(tf_vgg16 or pretrained_tf_model): label = tf.one_hot(parsed_example['image/class/label'] -1, 1000, dtype=tf.float32)\n",
    "    else: label = tf.one_hot(parsed_example['image/class/label'] -1, 1000, dtype=tf.int8)\n",
    "    \n",
    "    return (image,label)\n",
    "\n",
    "\n",
    "\n",
    "def get_val_tf_dataset(times_to_run = 150, prefetch = True):\n",
    "    val_filenames = get_val_filenames()\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(val_filenames) #dataset containing the filenames\n",
    "\n",
    "    VAL_BATCH_SIZE = 32 * times_to_run\n",
    "    val_dataset = val_dataset.interleave(tf.data.TFRecordDataset, cycle_length=8, num_parallel_calls=8, deterministic=False)\n",
    "    val_dataset = val_dataset.map(parse_and_preprocess_eval_image, num_parallel_calls=8, deterministic=False)\n",
    "    val_dataset = val_dataset.batch(VAL_BATCH_SIZE)\n",
    "    if(prefetch): val_dataset = val_dataset.prefetch(1)\n",
    "    return val_dataset\n",
    "\n",
    "def get_train_tf_dataset(times_to_run = 100):\n",
    "    train_filenames = get_train_filenames()\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_filenames) #dataset containing the filenames\n",
    "\n",
    "    TRAIN_BATCH_SIZE = 32*times_to_run\n",
    "    train_dataset = train_dataset.shuffle(tf.shape(train_filenames,out_type=tf.int64)[0])\n",
    "    train_dataset = train_dataset.repeat()\n",
    "    train_dataset = train_dataset.interleave(tf.data.TFRecordDataset, cycle_length=8, num_parallel_calls=8, deterministic=False)\n",
    "    train_dataset = train_dataset.map( lambda x: parse_and_preprocess_training_image(x,False), num_parallel_calls=8, deterministic=False)\n",
    "    train_dataset = train_dataset.batch(TRAIN_BATCH_SIZE)\n",
    "    train_dataset = train_dataset.prefetch(1)\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg_pretrained_weights():\n",
    "\n",
    "    WEIGHTS_PATH = ('https://storage.googleapis.com/tensorflow/keras-applications/'\n",
    "                    'vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "    weights_path = data_utils.get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',WEIGHTS_PATH,cache_subdir='models',file_hash='64373286793e3c8b2b4e3219cbf3544b')\n",
    "    weight_file = h5py.File(weights_path, 'r')\n",
    "\n",
    "    #this is basicaly a dictionary which contains 2 levels of keys. Params are found on the lowest levels (weights,biases). Some of the second keys are empty.\n",
    "    first_lvl_keys = list(weight_file.keys())\n",
    "    vgg_weights = np.array([],float)\n",
    "    vgg_bias = np.array([],float)\n",
    "    for keys1 in first_lvl_keys:\n",
    "        second_lvl_keys = list(weight_file[keys1].keys())\n",
    "        if second_lvl_keys:\n",
    "            vgg_weights = np.concatenate( (vgg_weights, weight_file[keys1][second_lvl_keys[0]][:].flatten()) ) #list contains 16 numpy arrays which have shape(HWCN)\n",
    "            vgg_bias = np.concatenate( (vgg_bias, weight_file[keys1][second_lvl_keys[1]][:].flatten()) )\n",
    "\n",
    "    weight_file.close()\n",
    "    return tf.convert_to_tensor(vgg_weights,tf.float32), tf.convert_to_tensor(vgg_bias,tf.float32)\n",
    "\n",
    "def win_transform_weights(vgg_weights):\n",
    "    with tf.device('/cpu:0'):\n",
    "        vgg_weights = vgg16_module.vgg16_weight_trans(vgg_weights) #compute winograd weights\n",
    "\n",
    "    return vgg_weights\n",
    "\n",
    "\n",
    "def initialize_vgg16_weights():\n",
    "    he_init = tf.keras.initializers.he_normal() # assumes the weight format is (...,C,N) for conv weights and (C,N) for fc weights\n",
    "    glorot_uni = tf.keras.initializers.GlorotUniform()\n",
    "    \n",
    "    weights = tf.reshape(he_init(shape=(3,3,3,64)),[-1])\n",
    "    weights = tf.concat( [weights,tf.reshape(he_init((3,3,64,64)),[-1])],axis=0 )\n",
    "    weights = tf.concat( [weights,tf.reshape(he_init((3,3,64,128)),[-1])],axis=0 )\n",
    "    weights = tf.concat( [weights,tf.reshape(he_init((3,3,128,128)),[-1])],axis=0 )\n",
    "    weights = tf.concat( [weights,tf.reshape(he_init((3,3,128,256)),[-1])],axis=0 )\n",
    "    weights = tf.concat( [weights,tf.reshape(he_init((3,3,256,256)),[-1])],axis=0 )\n",
    "    weights = tf.concat( [weights,tf.reshape(he_init((3,3,256,256)),[-1])],axis=0 )\n",
    "    weights = tf.concat( [weights,tf.reshape(he_init((3,3,256,512)),[-1])],axis=0 )\n",
    "    weights = tf.concat( [weights,tf.reshape(he_init((3,3,512,512)),[-1])],axis=0 )\n",
    "    weights = tf.concat( [weights,tf.reshape(he_init((3,3,512,512)),[-1])],axis=0 )\n",
    "    weights = tf.concat( [weights,tf.reshape(he_init((3,3,512,512)),[-1])],axis=0 )\n",
    "    weights = tf.concat( [weights,tf.reshape(he_init((3,3,512,512)),[-1])],axis=0 )\n",
    "    weights = tf.concat( [weights,tf.reshape(he_init((3,3,512,512)),[-1])],axis=0 )\n",
    "    weights = tf.concat( [weights,tf.reshape(he_init((25088,4096)),[-1])],axis=0 )\n",
    "    weights = tf.concat( [weights,tf.reshape(he_init((4096,4096)),[-1])],axis=0 )\n",
    "    \n",
    "    weights = tf.concat( [weights,tf.reshape(glorot_uni([4096,1000]),[-1])],axis=0 )\n",
    "    \n",
    "    weights_vel = tf.zeros([138344128],tf.float32) # 138344128 = total weight params\n",
    "    bias = tf.zeros([13416],tf.float32) # 13416 = total bias params\n",
    "    bias_vel = tf.zeros([13416],tf.float32)\n",
    "    \n",
    "    return tf.Variable(weights),tf.Variable(weights_vel),tf.Variable(bias),tf.Variable(bias_vel)\n",
    "\n",
    "def get_train_weights(initial_weights,w_filename):\n",
    "    if initial_weights == 'pre':\n",
    "        weights,bias = get_vgg_pretrained_weights()\n",
    "        bias_vel = tf.zeros([13416],tf.float32)\n",
    "        weights_vel = tf.zeros([138344128],tf.float32)\n",
    "    elif initial_weights == 'file':\n",
    "        weights,weights_vel,bias,bias_vel = read_weights_from_file(w_filename)\n",
    "    elif initial_weights == 'new':\n",
    "        weights,weights_vel,bias,bias_vel = initialize_vgg16_weights()\n",
    "    else: raise ValueError(\"initial weights arg not recognized (pre,file,new)\")\n",
    "\n",
    "    return weights,bias,weights_vel,bias_vel\n",
    "\n",
    "def get_val_weights(initial_weights,filename = None):\n",
    "    if initial_weights == 'pre':\n",
    "        weights,bias = get_vgg_pretrained_weights()\n",
    "    elif initial_weights == 'file':\n",
    "        weights,_,bias,_ = read_weights_from_file(filename)\n",
    "    else: raise ValueError(\"initial weights arg not recognized (pre,file)\")\n",
    "\n",
    "    return weights,bias\n",
    "\n",
    "def get_accuracy(labels,guesses,N,H): #labels(N*H) guesses(N)\n",
    "    correct = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        if (labels[ i*H + guesses[i] ] == 1): correct += 1\n",
    "    return correct/N\n",
    "\n",
    "#@tf.function\n",
    "def vgg16_inference(val_dataset,initial_weights,filename = None):\n",
    "\n",
    "    weights, bias = get_val_weights(initial_weights,filename)\n",
    "\n",
    "    labels = tf.zeros([0],tf.int8)\n",
    "    guesses_tensor = tf.zeros([0],tf.int32)\n",
    "    \n",
    "    weights = vgg16_module.vgg16_weight_trans(weights) # get winograd weights\n",
    "    \n",
    "    for vgg_input in val_dataset:\n",
    "        labels = tf.concat( [labels,tf.reshape(vgg_input[1],[-1])],axis=0)\n",
    "        times_to_run = tf.cast(tf.shape(vgg_input[0])[0]/32,tf.int32)\n",
    "        \n",
    "        guesses = vgg16_module.vgg16_custom_infer( vgg_input[0],weights,bias,times_to_run)\n",
    "        \n",
    "        guesses_tensor = tf.concat([guesses_tensor,guesses],axis=0)\n",
    "    \n",
    "    return guesses_tensor,labels\n",
    "\n",
    "\n",
    "def vgg16_custom_training(  train_dataset,\n",
    "                            val_dataset,\n",
    "                            weights,\n",
    "                            weights_vel,\n",
    "                            bias,\n",
    "                            bias_vel,\n",
    "                            lr,\n",
    "                            momentum,\n",
    "                            reg,\n",
    "                            times_to_run,\n",
    "                            out_mode,\n",
    "                            steps,\n",
    "                            save_filename):\n",
    "\n",
    "    current_step = 0\n",
    "    rolling_acc = 0\n",
    "    \n",
    "    for batch in train_dataset:\n",
    "       \n",
    "        weights,weights_vel,bias,bias_vel,loss,acc = vgg16_module.vgg16_custom_train_normal(batch[0],\n",
    "                                                                                            batch[1],\n",
    "                                                                                            weights,\n",
    "                                                                                            weights_vel,\n",
    "                                                                                            bias,\n",
    "                                                                                            bias_vel,\n",
    "                                                                                            reg,\n",
    "                                                                                            momentum,\n",
    "                                                                                            lr,\n",
    "                                                                                            times_to_run,\n",
    "                                                                                            out_mode\n",
    "                                                                                            )\n",
    "        \n",
    "        current_step +=1\n",
    "        \n",
    "        if rolling_acc == 0:\n",
    "            rolling_acc = acc\n",
    "        else:\n",
    "            rolling_acc = 0.95*rolling_acc + 0.05*acc\n",
    "\n",
    "        print(\"loss: \" + str(loss[0].numpy()) + \" categorical accuracy: \" + str(rolling_acc[0].numpy()), end=\"\\r\", flush=True)\n",
    "        \n",
    "        if (current_step >= steps): \n",
    "            save_weights_to_file(weights,weights_vel,bias,bias_vel)\n",
    "            print(\"categorical accuracy: \" + str(rolling_acc[0].numpy()))\n",
    "            guesses_tensor,labels = vgg16_inference(val_dataset,weights,bias)\n",
    "            print(get_accuracy(labels,guesses_tensor, tf.shape(guesses_tensor)[0], 1000))\n",
    "            return weights,weights_vel,bias,bias_vel\n",
    "\n",
    "        if ( (current_step * times_to_run) % 37500 == 0 ): save_weights_to_file(save_filename,weights,weights_vel,bias,bias_vel)\n",
    "\n",
    "        if ( (current_step * times_to_run) % 18800 == 0 ):\n",
    "            print(\"categorical accuracy: \" + str(rolling_acc[0].numpy()))\n",
    "            guesses_tensor,labels = vgg16_inference(val_dataset,weights,bias)\n",
    "            print(get_accuracy(labels,guesses_tensor, tf.shape(guesses_tensor)[0], 1000))\n",
    "        \n",
    "\n",
    "                                  \n",
    "def train_custom_vgg(train_dataset,val_dataset,times_to_run,initial_weights,epochs,lr,save_filename,load_filename=None):\n",
    "    \n",
    "    weights, bias, weights_vel, bias_vel = get_train_weights(initial_weights,load_filename)\n",
    "    \n",
    "    steps = int((epochs*37500)/times_to_run) #one epoch is around 1.2kk images so 37500 batches of 32\n",
    "    \n",
    "    weights,weights_vel,bias,bias_vel = vgg16_custom_training(  train_dataset,\n",
    "                                                                val_dataset,\n",
    "                                                                weights,\n",
    "                                                                weights_vel,\n",
    "                                                                bias,\n",
    "                                                                bias_vel,\n",
    "                                                                lr=lr,\n",
    "                                                                momentum=0.9,\n",
    "                                                                reg=4e-4,\n",
    "                                                                times_to_run=times_to_run,\n",
    "                                                                out_mode=2,\n",
    "                                                                steps=steps,\n",
    "                                                                save_filename=save_filename)\n",
    "    \n",
    "    return weights,weights_vel,bias,bias_vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference run\n",
    "\n",
    "weights_filename = \"weights filename\"\n",
    "\n",
    "val_dataset = get_val_tf_dataset(times_to_run = 150)\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    guesses_tensor,labels = vgg16_inference(val_dataset,\"file\",weights_filename)\n",
    "    print(get_accuracy(labels,guesses_tensor, tf.shape(guesses_tensor)[0], 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example training run\n",
    "\n",
    "save_filename = \"save filename\"\n",
    "load_filename = \"load filename\"\n",
    "\n",
    "times_to_run_train = 100 # times to run c++ code before returning (higher numbers require more RAM but reduce io overhead)\n",
    "train_dataset = get_train_tf_dataset(times_to_run_train)\n",
    "val_dataset = get_val_tf_dataset(times_to_run = 20, prefetch=False)\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    weights,weights_vel,bias,bias_vel = train_custom_vgg(   train_dataset,\n",
    "                                                            val_dataset,\n",
    "                                                            times_to_run_train,\n",
    "                                                            initial_weights='file',\n",
    "                                                            epochs=1,\n",
    "                                                            lr=8e-5, #5e-3 max\n",
    "                                                            load_filename = load_filename,\n",
    "                                                            save_filename = save_filename\n",
    "                                                            ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41568a918cf9b267773da6e3499cdcf8813886180c0110733d1e565d68ab3223"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('imagenet': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "0a1f393f61c3eabaa13bf17c0939436f3913f6146e185e3947d38a9c309ef6d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
